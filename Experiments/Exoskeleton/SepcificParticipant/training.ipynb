{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IMU1_GyroX_Mean  IMU1_GyroX_StdDev  IMU1_GyroX_RMS  IMU1_GyroX_Amp  \\\n",
      "0       100.000000         100.000000      100.000000      100.000000   \n",
      "1         7.026579         198.259304      199.601392      184.295571   \n",
      "2     -7082.108609         568.164429      593.320009      727.671625   \n",
      "3      3618.245437        1537.254629     1540.972428     1855.355638   \n",
      "4     -1539.312493        2426.076723     2428.697877     2399.474310   \n",
      "\n",
      "   IMU1_GyroY_Mean  IMU1_GyroY_StdDev  IMU1_GyroY_RMS  IMU1_GyroY_Amp  \\\n",
      "0       100.000000         100.000000      100.000000      100.000000   \n",
      "1       610.020298         175.334986      178.265684      192.958297   \n",
      "2      9456.718907         610.892516      672.664608      744.159826   \n",
      "3      4828.685519        2177.145176     2187.176663     2466.309254   \n",
      "4     -5421.387863        4325.733672     4335.967864     4673.869608   \n",
      "\n",
      "   IMU1_GyroZ_Mean  IMU1_GyroZ_StdDev  ...  OFS_FIB_IMNF  EMG_EMG_Mean  \\\n",
      "0       100.000000         100.000000  ...      1.282506    100.000000   \n",
      "1       -30.466274         188.025193  ...      1.147393    107.150748   \n",
      "2      -744.252427         567.506829  ...      0.989183    247.044115   \n",
      "3      -638.476505        1500.476246  ...      0.912529    713.389659   \n",
      "4       173.236814        1786.550574  ...      0.880923   2214.542677   \n",
      "\n",
      "   EMG_EMG_StdDev  EMG_EMG_RMS  EMG_EMG_Amp  EMG_EMG_MNF  EMG_EMG_MDF  \\\n",
      "0      100.000000   100.000000   100.000000     0.293036     0.230724   \n",
      "1       71.239481   103.936460    81.656880     0.142451     0.181664   \n",
      "2      611.569045   306.910586   514.609792     0.385845     0.234438   \n",
      "3     1127.501511   767.921875   968.529167     0.426387     0.192446   \n",
      "4     4157.944918  2493.259693  3969.429341     0.501780     0.220765   \n",
      "\n",
      "   EMG_EMG_IMNF  Fatigue_level  Subject  \n",
      "0     22.334139              1        1  \n",
      "1      8.384712              1        1  \n",
      "2      8.372799              1        1  \n",
      "3      7.684195              1        1  \n",
      "4      7.496121              1        1  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取原始CSV文件\n",
    "original_df = pd.read_csv('/home/adam/FurtherResearch/Dataset/Exoskeleton/Original.csv')\n",
    "\n",
    "# 显示前几行以了解其结构\n",
    "print(original_df.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:34:31.438398508Z",
     "start_time": "2024-02-16T11:34:31.395278840Z"
    }
   },
   "id": "113125776582355a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_1.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_2.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_3.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_4.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_5.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_6.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_7.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_8.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_9.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_10.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_11.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_12.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_13.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_14.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_15.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_16.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_17.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_18.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_19.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_20.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_21.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_22.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_23.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_24.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_25.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_26.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_27.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_28.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_29.csv\n",
      "Saved: /home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_30.csv\n"
     ]
    }
   ],
   "source": [
    "# 移除Subject列为空的行\n",
    "cleaned_df = original_df.dropna(subset=['Subject'])\n",
    "\n",
    "# 将Subject列转换为整数，确保文件命名一致\n",
    "cleaned_df['Subject'] = cleaned_df['Subject'].astype(int)\n",
    "\n",
    "# 为每个参试者创建和保存一个新的CSV文件\n",
    "for subject_id, data in cleaned_df.groupby('Subject'):\n",
    "    # 创建文件名\n",
    "    filename = f'/home/adam/FurtherResearch/Dataset/Exoskeleton/Sepcific/Subject_{subject_id}.csv'\n",
    "    # 保存CSV文件\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f'Saved: {filename}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:34:38.076489038Z",
     "start_time": "2024-02-16T11:34:37.952951748Z"
    }
   },
   "id": "b6d03567abfea4e0",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebdbe21c431b276",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:01.585069459Z",
     "start_time": "2024-02-16T11:43:55.202204050Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:43:57.132287: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU is available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:44:01.482238: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-02-16 11:44:01.482700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-02-16 11:44:01.522270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:01.522384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.78GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2024-02-16 11:44:01.522398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-16 11:44:01.539559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-16 11:44:01.539607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-02-16 11:44:01.548879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-16 11:44:01.558503: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-16 11:44:01.567625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-16 11:44:01.579288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-16 11:44:01.581445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-16 11:44:01.581553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:01.581678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:01.581746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Physical devices:\", physical_devices)\n",
    "\n",
    "# Check for GPU specifically\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is NOT available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n100    2\n101    2\n102    2\n103    2\n104    2\nName: Fatigue_level, Length: 105, dtype: int64"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "dataset = pd.read_csv('../../../Dataset/Exoskeleton/Specific/Subject_12.csv')  # Replace with your dataset path\n",
    "dataset.iloc[:, -1] = dataset.iloc[:, -1] - 1\n",
    "X = dataset.iloc[:, :-2]  # Features\n",
    "y = dataset.iloc[:, -2]-1 # Target\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:14.930109431Z",
     "start_time": "2024-02-16T11:44:14.913540920Z"
    }
   },
   "id": "7a4346fb480f0f57"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:44:18.210525: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 11:44:18.211585: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-02-16 11:44:18.211719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:18.211819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.78GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2024-02-16 11:44:18.211838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-16 11:44:18.211855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-16 11:44:18.211866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-02-16 11:44:18.211877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-16 11:44:18.211887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-16 11:44:18.211898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-16 11:44:18.211908: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-16 11:44:18.211919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-16 11:44:18.211959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:18.212057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:18.212132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2024-02-16 11:44:18.212152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-16 11:44:18.558595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-02-16 11:44:18.558616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2024-02-16 11:44:18.558620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2024-02-16 11:44:18.558775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:18.558912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:18.559012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-02-16 11:44:18.559089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6523 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_categorical = to_categorical(y)\n",
    "\n",
    "# Shuffle the data\n",
    "X_scaled, y_categorical = shuffle(X_scaled, y_categorical, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Number of unique classes in the target variable\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "\n",
    "# Building the DNN model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],), name='dense_0'),\n",
    "    Dense(64, activation='relu', name='dense_1'),\n",
    "    Dense(3, activation='relu', name='dense_2'),\n",
    "    # Dense(3, activation='relu', name='dense_3'),\n",
    "    Dense(num_classes, activation='softmax')  # Output layer\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:18.629105939Z",
     "start_time": "2024-02-16T11:44:17.721773206Z"
    }
   },
   "id": "1e58074cece23a2d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,  # minimium amount of change to count as an improvement\n",
    "    patience=100,  # how many epochs to wait before stopping\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create the ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.9,  # factor by which the learning rate will be reduced\n",
    "    patience=5,  # number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=0.00001,  # lower bound on the learning rate\n",
    "    verbose=1\n",
    ")\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(reduce_lr)\n",
    "\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.learning_rates = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'numpy'):\n",
    "            lr = lr.numpy()\n",
    "        elif hasattr(lr, 'eval'):\n",
    "            lr = lr.eval(session=tf.compat.v1.Session())\n",
    "        print(f'Epoch {epoch+1}: Learning rate is {lr}.')\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Retrieve the current learning rate from the optimizer\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "        # Append the current learning rate to the list\n",
    "        self.learning_rates.append(current_lr)\n",
    "\n",
    "lr_tracker = PrintLR()\n",
    "callbacks = [lr_tracker, early_stopping, reduce_lr]\n",
    "\n",
    "\n",
    "# Initialize the optimizer with the learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:26.626449815Z",
     "start_time": "2024-02-16T11:44:26.579831533Z"
    }
   },
   "id": "111da32ccd0d31d1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:44:30.032686: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2024-02-16 11:44:30.069299: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899885000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Epoch 1: Learning rate is 0.009999999776482582.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:44:30.694869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 408ms/step - loss: 0.9707 - accuracy: 0.4950 - val_loss: 0.8349 - val_accuracy: 0.8235\n",
      "Epoch 2/1000\n",
      "Epoch 2: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5832 - accuracy: 0.8484 - val_loss: 0.8251 - val_accuracy: 0.8235\n",
      "Epoch 3/1000\n",
      "Epoch 3: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5383 - accuracy: 0.8484 - val_loss: 0.8200 - val_accuracy: 0.8235\n",
      "Epoch 4/1000\n",
      "Epoch 4: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5288 - accuracy: 0.8333 - val_loss: 0.8095 - val_accuracy: 0.8235\n",
      "Epoch 5/1000\n",
      "Epoch 5: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5155 - accuracy: 0.8484 - val_loss: 0.8005 - val_accuracy: 0.8235\n",
      "Epoch 6/1000\n",
      "Epoch 6: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5146 - accuracy: 0.8536 - val_loss: 0.7956 - val_accuracy: 0.8235\n",
      "Epoch 7/1000\n",
      "Epoch 7: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5150 - accuracy: 0.8484 - val_loss: 0.7938 - val_accuracy: 0.8235\n",
      "Epoch 8/1000\n",
      "Epoch 8: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5108 - accuracy: 0.8484 - val_loss: 0.7658 - val_accuracy: 0.8235\n",
      "Epoch 9/1000\n",
      "Epoch 9: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5044 - accuracy: 0.8484 - val_loss: 0.7566 - val_accuracy: 0.8235\n",
      "Epoch 10/1000\n",
      "Epoch 10: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4932 - accuracy: 0.8536 - val_loss: 0.7477 - val_accuracy: 0.8235\n",
      "Epoch 11/1000\n",
      "Epoch 11: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4882 - accuracy: 0.8536 - val_loss: 0.7393 - val_accuracy: 0.8235\n",
      "Epoch 12/1000\n",
      "Epoch 12: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4897 - accuracy: 0.8484 - val_loss: 0.7312 - val_accuracy: 0.8235\n",
      "Epoch 13/1000\n",
      "Epoch 13: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4851 - accuracy: 0.8484 - val_loss: 0.7232 - val_accuracy: 0.8235\n",
      "Epoch 14/1000\n",
      "Epoch 14: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4806 - accuracy: 0.8484 - val_loss: 0.7153 - val_accuracy: 0.8235\n",
      "Epoch 15/1000\n",
      "Epoch 15: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4807 - accuracy: 0.8484 - val_loss: 0.7079 - val_accuracy: 0.8235\n",
      "Epoch 16/1000\n",
      "Epoch 16: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4703 - accuracy: 0.8536 - val_loss: 0.7014 - val_accuracy: 0.8235\n",
      "Epoch 17/1000\n",
      "Epoch 17: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4728 - accuracy: 0.8484 - val_loss: 0.6956 - val_accuracy: 0.8235\n",
      "Epoch 18/1000\n",
      "Epoch 18: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4630 - accuracy: 0.8536 - val_loss: 0.6904 - val_accuracy: 0.8235\n",
      "Epoch 19/1000\n",
      "Epoch 19: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4577 - accuracy: 0.8484 - val_loss: 0.6851 - val_accuracy: 0.8235\n",
      "Epoch 20/1000\n",
      "Epoch 20: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4525 - accuracy: 0.8536 - val_loss: 0.6794 - val_accuracy: 0.8235\n",
      "Epoch 21/1000\n",
      "Epoch 21: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4450 - accuracy: 0.8536 - val_loss: 0.6736 - val_accuracy: 0.8235\n",
      "Epoch 22/1000\n",
      "Epoch 22: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4520 - accuracy: 0.8484 - val_loss: 0.6676 - val_accuracy: 0.8235\n",
      "Epoch 23/1000\n",
      "Epoch 23: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4381 - accuracy: 0.8536 - val_loss: 0.6614 - val_accuracy: 0.8235\n",
      "Epoch 24/1000\n",
      "Epoch 24: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4387 - accuracy: 0.8536 - val_loss: 0.6554 - val_accuracy: 0.8235\n",
      "Epoch 25/1000\n",
      "Epoch 25: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4391 - accuracy: 0.8536 - val_loss: 0.6501 - val_accuracy: 0.8235\n",
      "Epoch 26/1000\n",
      "Epoch 26: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4344 - accuracy: 0.8484 - val_loss: 0.6450 - val_accuracy: 0.8235\n",
      "Epoch 27/1000\n",
      "Epoch 27: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4313 - accuracy: 0.8484 - val_loss: 0.6392 - val_accuracy: 0.8235\n",
      "Epoch 28/1000\n",
      "Epoch 28: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4294 - accuracy: 0.8536 - val_loss: 0.6338 - val_accuracy: 0.8235\n",
      "Epoch 29/1000\n",
      "Epoch 29: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4213 - accuracy: 0.8484 - val_loss: 0.6284 - val_accuracy: 0.8235\n",
      "Epoch 30/1000\n",
      "Epoch 30: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4221 - accuracy: 0.8484 - val_loss: 0.6225 - val_accuracy: 0.8235\n",
      "Epoch 31/1000\n",
      "Epoch 31: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4200 - accuracy: 0.8536 - val_loss: 0.6170 - val_accuracy: 0.8235\n",
      "Epoch 32/1000\n",
      "Epoch 32: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4125 - accuracy: 0.8484 - val_loss: 0.6117 - val_accuracy: 0.8235\n",
      "Epoch 33/1000\n",
      "Epoch 33: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4133 - accuracy: 0.8484 - val_loss: 0.6061 - val_accuracy: 0.8235\n",
      "Epoch 34/1000\n",
      "Epoch 34: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4105 - accuracy: 0.8484 - val_loss: 0.6004 - val_accuracy: 0.8235\n",
      "Epoch 35/1000\n",
      "Epoch 35: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4046 - accuracy: 0.8536 - val_loss: 0.5950 - val_accuracy: 0.8235\n",
      "Epoch 36/1000\n",
      "Epoch 36: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4084 - accuracy: 0.8484 - val_loss: 0.5901 - val_accuracy: 0.8235\n",
      "Epoch 37/1000\n",
      "Epoch 37: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4059 - accuracy: 0.8484 - val_loss: 0.5856 - val_accuracy: 0.8235\n",
      "Epoch 38/1000\n",
      "Epoch 38: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4004 - accuracy: 0.8484 - val_loss: 0.5810 - val_accuracy: 0.8235\n",
      "Epoch 39/1000\n",
      "Epoch 39: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4014 - accuracy: 0.8484 - val_loss: 0.5766 - val_accuracy: 0.8235\n",
      "Epoch 40/1000\n",
      "Epoch 40: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3961 - accuracy: 0.8484 - val_loss: 0.5722 - val_accuracy: 0.8235\n",
      "Epoch 41/1000\n",
      "Epoch 41: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3972 - accuracy: 0.8484 - val_loss: 0.5680 - val_accuracy: 0.8235\n",
      "Epoch 42/1000\n",
      "Epoch 42: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3885 - accuracy: 0.8536 - val_loss: 0.5642 - val_accuracy: 0.8235\n",
      "Epoch 43/1000\n",
      "Epoch 43: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3905 - accuracy: 0.8484 - val_loss: 0.5606 - val_accuracy: 0.8235\n",
      "Epoch 44/1000\n",
      "Epoch 44: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3878 - accuracy: 0.8536 - val_loss: 0.5573 - val_accuracy: 0.8235\n",
      "Epoch 45/1000\n",
      "Epoch 45: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3901 - accuracy: 0.8484 - val_loss: 0.5544 - val_accuracy: 0.8235\n",
      "Epoch 46/1000\n",
      "Epoch 46: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3858 - accuracy: 0.8484 - val_loss: 0.5514 - val_accuracy: 0.8235\n",
      "Epoch 47/1000\n",
      "Epoch 47: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3802 - accuracy: 0.8536 - val_loss: 0.5485 - val_accuracy: 0.8235\n",
      "Epoch 48/1000\n",
      "Epoch 48: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3759 - accuracy: 0.8536 - val_loss: 0.5459 - val_accuracy: 0.8235\n",
      "Epoch 49/1000\n",
      "Epoch 49: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3843 - accuracy: 0.8484 - val_loss: 0.5432 - val_accuracy: 0.8235\n",
      "Epoch 50/1000\n",
      "Epoch 50: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3828 - accuracy: 0.8484 - val_loss: 0.5407 - val_accuracy: 0.8235\n",
      "Epoch 51/1000\n",
      "Epoch 51: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3744 - accuracy: 0.8536 - val_loss: 0.5383 - val_accuracy: 0.8235\n",
      "Epoch 52/1000\n",
      "Epoch 52: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3758 - accuracy: 0.8536 - val_loss: 0.5363 - val_accuracy: 0.8235\n",
      "Epoch 53/1000\n",
      "Epoch 53: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3790 - accuracy: 0.8484 - val_loss: 0.5346 - val_accuracy: 0.8235\n",
      "Epoch 54/1000\n",
      "Epoch 54: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3779 - accuracy: 0.8484 - val_loss: 0.5327 - val_accuracy: 0.8235\n",
      "Epoch 55/1000\n",
      "Epoch 55: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3768 - accuracy: 0.8484 - val_loss: 0.5308 - val_accuracy: 0.8235\n",
      "Epoch 56/1000\n",
      "Epoch 56: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3757 - accuracy: 0.8484 - val_loss: 0.5289 - val_accuracy: 0.8235\n",
      "Epoch 57/1000\n",
      "Epoch 57: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3695 - accuracy: 0.8484 - val_loss: 0.5267 - val_accuracy: 0.8235\n",
      "Epoch 58/1000\n",
      "Epoch 58: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3737 - accuracy: 0.8484 - val_loss: 0.5243 - val_accuracy: 0.8235\n",
      "Epoch 59/1000\n",
      "Epoch 59: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3630 - accuracy: 0.8536 - val_loss: 0.5221 - val_accuracy: 0.8235\n",
      "Epoch 60/1000\n",
      "Epoch 60: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3670 - accuracy: 0.8536 - val_loss: 0.5202 - val_accuracy: 0.8235\n",
      "Epoch 61/1000\n",
      "Epoch 61: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3707 - accuracy: 0.8484 - val_loss: 0.5185 - val_accuracy: 0.8235\n",
      "Epoch 62/1000\n",
      "Epoch 62: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3697 - accuracy: 0.8484 - val_loss: 0.5168 - val_accuracy: 0.8235\n",
      "Epoch 63/1000\n",
      "Epoch 63: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3688 - accuracy: 0.8484 - val_loss: 0.5151 - val_accuracy: 0.8235\n",
      "Epoch 64/1000\n",
      "Epoch 64: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3680 - accuracy: 0.8484 - val_loss: 0.5135 - val_accuracy: 0.8235\n",
      "Epoch 65/1000\n",
      "Epoch 65: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3576 - accuracy: 0.8536 - val_loss: 0.5118 - val_accuracy: 0.8235\n",
      "Epoch 66/1000\n",
      "Epoch 66: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3664 - accuracy: 0.8484 - val_loss: 0.5102 - val_accuracy: 0.8235\n",
      "Epoch 67/1000\n",
      "Epoch 67: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3607 - accuracy: 0.8536 - val_loss: 0.5088 - val_accuracy: 0.8235\n",
      "Epoch 68/1000\n",
      "Epoch 68: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3526 - accuracy: 0.8588 - val_loss: 0.5078 - val_accuracy: 0.8235\n",
      "Epoch 69/1000\n",
      "Epoch 69: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3640 - accuracy: 0.8484 - val_loss: 0.5071 - val_accuracy: 0.8235\n",
      "Epoch 70/1000\n",
      "Epoch 70: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3487 - accuracy: 0.8588 - val_loss: 0.5065 - val_accuracy: 0.8235\n",
      "Epoch 71/1000\n",
      "Epoch 71: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3528 - accuracy: 0.8536 - val_loss: 0.5060 - val_accuracy: 0.8235\n",
      "Epoch 72/1000\n",
      "Epoch 72: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3615 - accuracy: 0.8484 - val_loss: 0.5053 - val_accuracy: 0.8235\n",
      "Epoch 73/1000\n",
      "Epoch 73: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3465 - accuracy: 0.8588 - val_loss: 0.5047 - val_accuracy: 0.8235\n",
      "Epoch 74/1000\n",
      "Epoch 74: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3551 - accuracy: 0.8484 - val_loss: 0.5040 - val_accuracy: 0.8235\n",
      "Epoch 75/1000\n",
      "Epoch 75: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3590 - accuracy: 0.8484 - val_loss: 0.5030 - val_accuracy: 0.8235\n",
      "Epoch 76/1000\n",
      "Epoch 76: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3582 - accuracy: 0.8484 - val_loss: 0.5018 - val_accuracy: 0.8235\n",
      "Epoch 77/1000\n",
      "Epoch 77: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3529 - accuracy: 0.8536 - val_loss: 0.5008 - val_accuracy: 0.8235\n",
      "Epoch 78/1000\n",
      "Epoch 78: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3591 - accuracy: 0.8484 - val_loss: 0.5000 - val_accuracy: 0.8235\n",
      "Epoch 79/1000\n",
      "Epoch 79: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3585 - accuracy: 0.8484 - val_loss: 0.4993 - val_accuracy: 0.8235\n",
      "Epoch 80/1000\n",
      "Epoch 80: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3487 - accuracy: 0.8536 - val_loss: 0.4987 - val_accuracy: 0.8235\n",
      "Epoch 81/1000\n",
      "Epoch 81: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3481 - accuracy: 0.8536 - val_loss: 0.4982 - val_accuracy: 0.8235\n",
      "Epoch 82/1000\n",
      "Epoch 82: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3452 - accuracy: 0.8536 - val_loss: 0.4977 - val_accuracy: 0.8235\n",
      "Epoch 83/1000\n",
      "Epoch 83: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3539 - accuracy: 0.8484 - val_loss: 0.4970 - val_accuracy: 0.8235\n",
      "Epoch 84/1000\n",
      "Epoch 84: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3556 - accuracy: 0.8484 - val_loss: 0.4963 - val_accuracy: 0.8235\n",
      "Epoch 85/1000\n",
      "Epoch 85: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3527 - accuracy: 0.8484 - val_loss: 0.4955 - val_accuracy: 0.8235\n",
      "Epoch 86/1000\n",
      "Epoch 86: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3474 - accuracy: 0.8484 - val_loss: 0.4944 - val_accuracy: 0.8235\n",
      "Epoch 87/1000\n",
      "Epoch 87: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3492 - accuracy: 0.8484 - val_loss: 0.4928 - val_accuracy: 0.8235\n",
      "Epoch 88/1000\n",
      "Epoch 88: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3510 - accuracy: 0.8484 - val_loss: 0.4912 - val_accuracy: 0.8235\n",
      "Epoch 89/1000\n",
      "Epoch 89: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3481 - accuracy: 0.8484 - val_loss: 0.4896 - val_accuracy: 0.8235\n",
      "Epoch 90/1000\n",
      "Epoch 90: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3453 - accuracy: 0.8484 - val_loss: 0.4878 - val_accuracy: 0.8235\n",
      "Epoch 91/1000\n",
      "Epoch 91: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3448 - accuracy: 0.8536 - val_loss: 0.4861 - val_accuracy: 0.8235\n",
      "Epoch 92/1000\n",
      "Epoch 92: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3489 - accuracy: 0.8484 - val_loss: 0.4848 - val_accuracy: 0.8235\n",
      "Epoch 93/1000\n",
      "Epoch 93: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3463 - accuracy: 0.8484 - val_loss: 0.4835 - val_accuracy: 0.8235\n",
      "Epoch 94/1000\n",
      "Epoch 94: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3480 - accuracy: 0.8484 - val_loss: 0.4821 - val_accuracy: 0.8235\n",
      "Epoch 95/1000\n",
      "Epoch 95: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3428 - accuracy: 0.8536 - val_loss: 0.4810 - val_accuracy: 0.8235\n",
      "Epoch 96/1000\n",
      "Epoch 96: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3473 - accuracy: 0.8484 - val_loss: 0.4801 - val_accuracy: 0.8235\n",
      "Epoch 97/1000\n",
      "Epoch 97: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3348 - accuracy: 0.8588 - val_loss: 0.4794 - val_accuracy: 0.8235\n",
      "Epoch 98/1000\n",
      "Epoch 98: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3465 - accuracy: 0.8484 - val_loss: 0.4790 - val_accuracy: 0.8235\n",
      "Epoch 99/1000\n",
      "Epoch 99: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3439 - accuracy: 0.8484 - val_loss: 0.4784 - val_accuracy: 0.8235\n",
      "Epoch 100/1000\n",
      "Epoch 100: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3435 - accuracy: 0.8484 - val_loss: 0.4776 - val_accuracy: 0.8235\n",
      "Epoch 101/1000\n",
      "Epoch 101: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3331 - accuracy: 0.8588 - val_loss: 0.4770 - val_accuracy: 0.8235\n",
      "Epoch 102/1000\n",
      "Epoch 102: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3448 - accuracy: 0.8484 - val_loss: 0.4766 - val_accuracy: 0.8235\n",
      "Epoch 103/1000\n",
      "Epoch 103: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3443 - accuracy: 0.8484 - val_loss: 0.4760 - val_accuracy: 0.8235\n",
      "Epoch 104/1000\n",
      "Epoch 104: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3389 - accuracy: 0.8536 - val_loss: 0.4756 - val_accuracy: 0.8235\n",
      "Epoch 105/1000\n",
      "Epoch 105: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3385 - accuracy: 0.8536 - val_loss: 0.4755 - val_accuracy: 0.8235\n",
      "Epoch 106/1000\n",
      "Epoch 106: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3431 - accuracy: 0.8484 - val_loss: 0.4754 - val_accuracy: 0.8235\n",
      "Epoch 107/1000\n",
      "Epoch 107: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3406 - accuracy: 0.8484 - val_loss: 0.4750 - val_accuracy: 0.8235\n",
      "Epoch 108/1000\n",
      "Epoch 108: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3402 - accuracy: 0.8484 - val_loss: 0.4744 - val_accuracy: 0.8235\n",
      "Epoch 109/1000\n",
      "Epoch 109: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3441 - accuracy: 0.8484 - val_loss: 0.4737 - val_accuracy: 0.8235\n",
      "Epoch 110/1000\n",
      "Epoch 110: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3437 - accuracy: 0.8484 - val_loss: 0.4731 - val_accuracy: 0.8235\n",
      "Epoch 111/1000\n",
      "Epoch 111: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3365 - accuracy: 0.8536 - val_loss: 0.4727 - val_accuracy: 0.8235\n",
      "Epoch 112/1000\n",
      "Epoch 112: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3340 - accuracy: 0.8536 - val_loss: 0.4724 - val_accuracy: 0.8235\n",
      "Epoch 113/1000\n",
      "Epoch 113: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3386 - accuracy: 0.8484 - val_loss: 0.4721 - val_accuracy: 0.8235\n",
      "Epoch 114/1000\n",
      "Epoch 114: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3383 - accuracy: 0.8484 - val_loss: 0.4716 - val_accuracy: 0.8235\n",
      "Epoch 115/1000\n",
      "Epoch 115: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3379 - accuracy: 0.8484 - val_loss: 0.4708 - val_accuracy: 0.8235\n",
      "Epoch 116/1000\n",
      "Epoch 116: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3356 - accuracy: 0.8484 - val_loss: 0.4697 - val_accuracy: 0.8235\n",
      "Epoch 117/1000\n",
      "Epoch 117: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3325 - accuracy: 0.8536 - val_loss: 0.4687 - val_accuracy: 0.8235\n",
      "Epoch 118/1000\n",
      "Epoch 118: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3392 - accuracy: 0.8484 - val_loss: 0.4679 - val_accuracy: 0.8235\n",
      "Epoch 119/1000\n",
      "Epoch 119: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3369 - accuracy: 0.8484 - val_loss: 0.4670 - val_accuracy: 0.8235\n",
      "Epoch 120/1000\n",
      "Epoch 120: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3317 - accuracy: 0.8536 - val_loss: 0.4661 - val_accuracy: 0.8235\n",
      "Epoch 121/1000\n",
      "Epoch 121: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3314 - accuracy: 0.8536 - val_loss: 0.4655 - val_accuracy: 0.8235\n",
      "Epoch 122/1000\n",
      "Epoch 122: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3331 - accuracy: 0.8536 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
      "Epoch 123/1000\n",
      "Epoch 123: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3359 - accuracy: 0.8484 - val_loss: 0.4648 - val_accuracy: 0.8235\n",
      "Epoch 124/1000\n",
      "Epoch 124: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3335 - accuracy: 0.8484 - val_loss: 0.4642 - val_accuracy: 0.8235\n",
      "Epoch 125/1000\n",
      "Epoch 125: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3392 - accuracy: 0.8484 - val_loss: 0.4635 - val_accuracy: 0.8235\n",
      "Epoch 126/1000\n",
      "Epoch 126: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3350 - accuracy: 0.8484 - val_loss: 0.4628 - val_accuracy: 0.8235\n",
      "Epoch 127/1000\n",
      "Epoch 127: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3297 - accuracy: 0.8536 - val_loss: 0.4621 - val_accuracy: 0.8235\n",
      "Epoch 128/1000\n",
      "Epoch 128: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3275 - accuracy: 0.8536 - val_loss: 0.4616 - val_accuracy: 0.8235\n",
      "Epoch 129/1000\n",
      "Epoch 129: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3311 - accuracy: 0.8536 - val_loss: 0.4613 - val_accuracy: 0.8235\n",
      "Epoch 130/1000\n",
      "Epoch 130: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3288 - accuracy: 0.8536 - val_loss: 0.4611 - val_accuracy: 0.8235\n",
      "Epoch 131/1000\n",
      "Epoch 131: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3304 - accuracy: 0.8536 - val_loss: 0.4611 - val_accuracy: 0.8235\n",
      "Epoch 132/1000\n",
      "Epoch 132: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3229 - accuracy: 0.8588 - val_loss: 0.4614 - val_accuracy: 0.8235\n",
      "Epoch 133/1000\n",
      "Epoch 133: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3296 - accuracy: 0.8536 - val_loss: 0.4621 - val_accuracy: 0.8235\n",
      "Epoch 134/1000\n",
      "Epoch 134: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3293 - accuracy: 0.8536 - val_loss: 0.4629 - val_accuracy: 0.8235\n",
      "Epoch 135/1000\n",
      "Epoch 135: Learning rate is 0.009999999776482582.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3250 - accuracy: 0.8536 - val_loss: 0.4637 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\n",
      "Epoch 136/1000\n",
      "Epoch 136: Learning rate is 0.008999999612569809.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3356 - accuracy: 0.8484 - val_loss: 0.4643 - val_accuracy: 0.8235\n",
      "Epoch 137/1000\n",
      "Epoch 137: Learning rate is 0.008999999612569809.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3333 - accuracy: 0.8484 - val_loss: 0.4647 - val_accuracy: 0.8235\n",
      "Epoch 138/1000\n",
      "Epoch 138: Learning rate is 0.008999999612569809.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3285 - accuracy: 0.8536 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
      "Epoch 139/1000\n",
      "Epoch 139: Learning rate is 0.008999999612569809.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3283 - accuracy: 0.8536 - val_loss: 0.4657 - val_accuracy: 0.8235\n",
      "Epoch 140/1000\n",
      "Epoch 140: Learning rate is 0.008999999612569809.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3305 - accuracy: 0.8484 - val_loss: 0.4662 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 0.008099999651312828.\n",
      "Epoch 141/1000\n",
      "Epoch 141: Learning rate is 0.008099999278783798.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3325 - accuracy: 0.8484 - val_loss: 0.4663 - val_accuracy: 0.8235\n",
      "Epoch 142/1000\n",
      "Epoch 142: Learning rate is 0.008099999278783798.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3302 - accuracy: 0.8484 - val_loss: 0.4661 - val_accuracy: 0.8235\n",
      "Epoch 143/1000\n",
      "Epoch 143: Learning rate is 0.008099999278783798.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3300 - accuracy: 0.8484 - val_loss: 0.4656 - val_accuracy: 0.8235\n",
      "Epoch 144/1000\n",
      "Epoch 144: Learning rate is 0.008099999278783798.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3342 - accuracy: 0.8484 - val_loss: 0.4650 - val_accuracy: 0.8235\n",
      "Epoch 145/1000\n",
      "Epoch 145: Learning rate is 0.008099999278783798.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3340 - accuracy: 0.8484 - val_loss: 0.4645 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.007289999350905419.\n",
      "Epoch 146/1000\n",
      "Epoch 146: Learning rate is 0.0072899991646409035.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3273 - accuracy: 0.8536 - val_loss: 0.4642 - val_accuracy: 0.8235\n",
      "Epoch 147/1000\n",
      "Epoch 147: Learning rate is 0.0072899991646409035.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3271 - accuracy: 0.8536 - val_loss: 0.4642 - val_accuracy: 0.8235\n",
      "Epoch 148/1000\n",
      "Epoch 148: Learning rate is 0.0072899991646409035.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3184 - accuracy: 0.8588 - val_loss: 0.4645 - val_accuracy: 0.8235\n",
      "Epoch 149/1000\n",
      "Epoch 149: Learning rate is 0.0072899991646409035.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3312 - accuracy: 0.8484 - val_loss: 0.4648 - val_accuracy: 0.8235\n",
      "Epoch 150/1000\n",
      "Epoch 150: Learning rate is 0.0072899991646409035.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3311 - accuracy: 0.8484 - val_loss: 0.4649 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.006560999248176813.\n",
      "Epoch 151/1000\n",
      "Epoch 151: Learning rate is 0.006560999434441328.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3288 - accuracy: 0.8484 - val_loss: 0.4648 - val_accuracy: 0.8235\n",
      "Epoch 152/1000\n",
      "Epoch 152: Learning rate is 0.006560999434441328.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3203 - accuracy: 0.8588 - val_loss: 0.4647 - val_accuracy: 0.8235\n",
      "Epoch 153/1000\n",
      "Epoch 153: Learning rate is 0.006560999434441328.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3266 - accuracy: 0.8536 - val_loss: 0.4650 - val_accuracy: 0.8235\n",
      "Epoch 154/1000\n",
      "Epoch 154: Learning rate is 0.006560999434441328.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3243 - accuracy: 0.8536 - val_loss: 0.4654 - val_accuracy: 0.8235\n",
      "Epoch 155/1000\n",
      "Epoch 155: Learning rate is 0.006560999434441328.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3220 - accuracy: 0.8536 - val_loss: 0.4657 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.005904899490997195.\n",
      "Epoch 156/1000\n",
      "Epoch 156: Learning rate is 0.0059048994444310665.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3242 - accuracy: 0.8536 - val_loss: 0.4660 - val_accuracy: 0.8235\n",
      "Epoch 157/1000\n",
      "Epoch 157: Learning rate is 0.0059048994444310665.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3219 - accuracy: 0.8536 - val_loss: 0.4662 - val_accuracy: 0.8235\n",
      "Epoch 158/1000\n",
      "Epoch 158: Learning rate is 0.0059048994444310665.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3303 - accuracy: 0.8484 - val_loss: 0.4663 - val_accuracy: 0.8235\n",
      "Epoch 159/1000\n",
      "Epoch 159: Learning rate is 0.0059048994444310665.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3302 - accuracy: 0.8484 - val_loss: 0.4662 - val_accuracy: 0.8235\n",
      "Epoch 160/1000\n",
      "Epoch 160: Learning rate is 0.0059048994444310665.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3301 - accuracy: 0.8484 - val_loss: 0.4660 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.00531440949998796.\n",
      "Epoch 161/1000\n",
      "Epoch 161: Learning rate is 0.0053144097328186035.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3323 - accuracy: 0.8484 - val_loss: 0.4657 - val_accuracy: 0.8235\n",
      "Epoch 162/1000\n",
      "Epoch 162: Learning rate is 0.0053144097328186035.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3259 - accuracy: 0.8536 - val_loss: 0.4656 - val_accuracy: 0.8235\n",
      "Epoch 163/1000\n",
      "Epoch 163: Learning rate is 0.0053144097328186035.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3298 - accuracy: 0.8484 - val_loss: 0.4655 - val_accuracy: 0.8235\n",
      "Epoch 164/1000\n",
      "Epoch 164: Learning rate is 0.0053144097328186035.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3297 - accuracy: 0.8484 - val_loss: 0.4653 - val_accuracy: 0.8235\n",
      "Epoch 165/1000\n",
      "Epoch 165: Learning rate is 0.0053144097328186035.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3234 - accuracy: 0.8536 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.004782968759536744.\n",
      "Epoch 166/1000\n",
      "Epoch 166: Learning rate is 0.004782968666404486.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3255 - accuracy: 0.8536 - val_loss: 0.4650 - val_accuracy: 0.8235\n",
      "Epoch 167/1000\n",
      "Epoch 167: Learning rate is 0.004782968666404486.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3317 - accuracy: 0.8484 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
      "Epoch 168/1000\n",
      "Epoch 168: Learning rate is 0.004782968666404486.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3271 - accuracy: 0.8484 - val_loss: 0.4650 - val_accuracy: 0.8235\n",
      "Epoch 169/1000\n",
      "Epoch 169: Learning rate is 0.004782968666404486.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3208 - accuracy: 0.8536 - val_loss: 0.4648 - val_accuracy: 0.8235\n",
      "Epoch 170/1000\n",
      "Epoch 170: Learning rate is 0.004782968666404486.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3269 - accuracy: 0.8484 - val_loss: 0.4644 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00170: ReduceLROnPlateau reducing learning rate to 0.0043046717997640375.\n",
      "Epoch 171/1000\n",
      "Epoch 171: Learning rate is 0.004304671660065651.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3313 - accuracy: 0.8484 - val_loss: 0.4641 - val_accuracy: 0.8235\n",
      "Epoch 172/1000\n",
      "Epoch 172: Learning rate is 0.004304671660065651.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3290 - accuracy: 0.8484 - val_loss: 0.4638 - val_accuracy: 0.8235\n",
      "Epoch 173/1000\n",
      "Epoch 173: Learning rate is 0.004304671660065651.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3289 - accuracy: 0.8484 - val_loss: 0.4634 - val_accuracy: 0.8235\n",
      "Epoch 174/1000\n",
      "Epoch 174: Learning rate is 0.004304671660065651.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3288 - accuracy: 0.8484 - val_loss: 0.4629 - val_accuracy: 0.8235\n",
      "Epoch 175/1000\n",
      "Epoch 175: Learning rate is 0.004304671660065651.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3246 - accuracy: 0.8536 - val_loss: 0.4626 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.0038742044940590858.\n",
      "Epoch 176/1000\n",
      "Epoch 176: Learning rate is 0.0038742045871913433.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3286 - accuracy: 0.8484 - val_loss: 0.4624 - val_accuracy: 0.8235\n",
      "Epoch 177/1000\n",
      "Epoch 177: Learning rate is 0.0038742045871913433.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3285 - accuracy: 0.8484 - val_loss: 0.4621 - val_accuracy: 0.8235\n",
      "Epoch 178/1000\n",
      "Epoch 178: Learning rate is 0.0038742045871913433.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3306 - accuracy: 0.8484 - val_loss: 0.4619 - val_accuracy: 0.8235\n",
      "Epoch 179/1000\n",
      "Epoch 179: Learning rate is 0.0038742045871913433.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3305 - accuracy: 0.8484 - val_loss: 0.4616 - val_accuracy: 0.8235\n",
      "Epoch 180/1000\n",
      "Epoch 180: Learning rate is 0.0038742045871913433.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3282 - accuracy: 0.8484 - val_loss: 0.4614 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.003486784128472209.\n",
      "Epoch 181/1000\n",
      "Epoch 181: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3304 - accuracy: 0.8484 - val_loss: 0.4611 - val_accuracy: 0.8235\n",
      "Epoch 182/1000\n",
      "Epoch 182: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3259 - accuracy: 0.8484 - val_loss: 0.4609 - val_accuracy: 0.8235\n",
      "Epoch 183/1000\n",
      "Epoch 183: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3259 - accuracy: 0.8484 - val_loss: 0.4605 - val_accuracy: 0.8235\n",
      "Epoch 184/1000\n",
      "Epoch 184: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3258 - accuracy: 0.8484 - val_loss: 0.4600 - val_accuracy: 0.8235\n",
      "Epoch 185/1000\n",
      "Epoch 185: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3279 - accuracy: 0.8484 - val_loss: 0.4596 - val_accuracy: 0.8235\n",
      "Epoch 186/1000\n",
      "Epoch 186: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3256 - accuracy: 0.8484 - val_loss: 0.4591 - val_accuracy: 0.8235\n",
      "Epoch 187/1000\n",
      "Epoch 187: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3235 - accuracy: 0.8536 - val_loss: 0.4587 - val_accuracy: 0.8235\n",
      "Epoch 188/1000\n",
      "Epoch 188: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3276 - accuracy: 0.8484 - val_loss: 0.4583 - val_accuracy: 0.8235\n",
      "Epoch 189/1000\n",
      "Epoch 189: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3276 - accuracy: 0.8484 - val_loss: 0.4580 - val_accuracy: 0.8235\n",
      "Epoch 190/1000\n",
      "Epoch 190: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3275 - accuracy: 0.8484 - val_loss: 0.4577 - val_accuracy: 0.8235\n",
      "Epoch 191/1000\n",
      "Epoch 191: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3274 - accuracy: 0.8484 - val_loss: 0.4573 - val_accuracy: 0.8235\n",
      "Epoch 192/1000\n",
      "Epoch 192: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3295 - accuracy: 0.8484 - val_loss: 0.4570 - val_accuracy: 0.8235\n",
      "Epoch 193/1000\n",
      "Epoch 193: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3231 - accuracy: 0.8484 - val_loss: 0.4567 - val_accuracy: 0.8235\n",
      "Epoch 194/1000\n",
      "Epoch 194: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3208 - accuracy: 0.8536 - val_loss: 0.4563 - val_accuracy: 0.8235\n",
      "Epoch 195/1000\n",
      "Epoch 195: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3272 - accuracy: 0.8484 - val_loss: 0.4560 - val_accuracy: 0.8235\n",
      "Epoch 196/1000\n",
      "Epoch 196: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3272 - accuracy: 0.8484 - val_loss: 0.4557 - val_accuracy: 0.8235\n",
      "Epoch 197/1000\n",
      "Epoch 197: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3292 - accuracy: 0.8484 - val_loss: 0.4554 - val_accuracy: 0.8235\n",
      "Epoch 198/1000\n",
      "Epoch 198: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3271 - accuracy: 0.8484 - val_loss: 0.4551 - val_accuracy: 0.8235\n",
      "Epoch 199/1000\n",
      "Epoch 199: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3270 - accuracy: 0.8484 - val_loss: 0.4549 - val_accuracy: 0.8235\n",
      "Epoch 200/1000\n",
      "Epoch 200: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3138 - accuracy: 0.8588 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "Epoch 201/1000\n",
      "Epoch 201: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3224 - accuracy: 0.8536 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "Epoch 202/1000\n",
      "Epoch 202: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3269 - accuracy: 0.8484 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "Epoch 203/1000\n",
      "Epoch 203: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3223 - accuracy: 0.8536 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "Epoch 204/1000\n",
      "Epoch 204: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3267 - accuracy: 0.8484 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "Epoch 205/1000\n",
      "Epoch 205: Learning rate is 0.0034867841750383377.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3246 - accuracy: 0.8484 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 0.003138105757534504.\n",
      "Epoch 206/1000\n",
      "Epoch 206: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3180 - accuracy: 0.8536 - val_loss: 0.4546 - val_accuracy: 0.8235\n",
      "Epoch 207/1000\n",
      "Epoch 207: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3200 - accuracy: 0.8536 - val_loss: 0.4545 - val_accuracy: 0.8235\n",
      "Epoch 208/1000\n",
      "Epoch 208: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3200 - accuracy: 0.8536 - val_loss: 0.4545 - val_accuracy: 0.8235\n",
      "Epoch 209/1000\n",
      "Epoch 209: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3199 - accuracy: 0.8536 - val_loss: 0.4545 - val_accuracy: 0.8235\n",
      "Epoch 210/1000\n",
      "Epoch 210: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3223 - accuracy: 0.8484 - val_loss: 0.4545 - val_accuracy: 0.8235\n",
      "Epoch 211/1000\n",
      "Epoch 211: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3263 - accuracy: 0.8484 - val_loss: 0.4543 - val_accuracy: 0.8235\n",
      "Epoch 212/1000\n",
      "Epoch 212: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3242 - accuracy: 0.8484 - val_loss: 0.4541 - val_accuracy: 0.8235\n",
      "Epoch 213/1000\n",
      "Epoch 213: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3262 - accuracy: 0.8484 - val_loss: 0.4539 - val_accuracy: 0.8235\n",
      "Epoch 214/1000\n",
      "Epoch 214: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3241 - accuracy: 0.8484 - val_loss: 0.4536 - val_accuracy: 0.8235\n",
      "Epoch 215/1000\n",
      "Epoch 215: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3216 - accuracy: 0.8536 - val_loss: 0.4534 - val_accuracy: 0.8235\n",
      "Epoch 216/1000\n",
      "Epoch 216: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3215 - accuracy: 0.8536 - val_loss: 0.4533 - val_accuracy: 0.8235\n",
      "Epoch 217/1000\n",
      "Epoch 217: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3174 - accuracy: 0.8536 - val_loss: 0.4532 - val_accuracy: 0.8235\n",
      "Epoch 218/1000\n",
      "Epoch 218: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3194 - accuracy: 0.8536 - val_loss: 0.4532 - val_accuracy: 0.8235\n",
      "Epoch 219/1000\n",
      "Epoch 219: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3259 - accuracy: 0.8484 - val_loss: 0.4532 - val_accuracy: 0.8235\n",
      "Epoch 220/1000\n",
      "Epoch 220: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3213 - accuracy: 0.8536 - val_loss: 0.4532 - val_accuracy: 0.8235\n",
      "Epoch 221/1000\n",
      "Epoch 221: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3279 - accuracy: 0.8484 - val_loss: 0.4533 - val_accuracy: 0.8235\n",
      "Epoch 222/1000\n",
      "Epoch 222: Learning rate is 0.0031381058506667614.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3278 - accuracy: 0.8484 - val_loss: 0.4533 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00222: ReduceLROnPlateau reducing learning rate to 0.0028242952656000854.\n",
      "Epoch 223/1000\n",
      "Epoch 223: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3278 - accuracy: 0.8484 - val_loss: 0.4533 - val_accuracy: 0.8235\n",
      "Epoch 224/1000\n",
      "Epoch 224: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3236 - accuracy: 0.8484 - val_loss: 0.4533 - val_accuracy: 0.8235\n",
      "Epoch 225/1000\n",
      "Epoch 225: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3256 - accuracy: 0.8484 - val_loss: 0.4532 - val_accuracy: 0.8235\n",
      "Epoch 226/1000\n",
      "Epoch 226: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3256 - accuracy: 0.8484 - val_loss: 0.4530 - val_accuracy: 0.8235\n",
      "Epoch 227/1000\n",
      "Epoch 227: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3235 - accuracy: 0.8484 - val_loss: 0.4528 - val_accuracy: 0.8235\n",
      "Epoch 228/1000\n",
      "Epoch 228: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3235 - accuracy: 0.8484 - val_loss: 0.4526 - val_accuracy: 0.8235\n",
      "Epoch 229/1000\n",
      "Epoch 229: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3210 - accuracy: 0.8536 - val_loss: 0.4524 - val_accuracy: 0.8235\n",
      "Epoch 230/1000\n",
      "Epoch 230: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3189 - accuracy: 0.8536 - val_loss: 0.4523 - val_accuracy: 0.8235\n",
      "Epoch 231/1000\n",
      "Epoch 231: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3274 - accuracy: 0.8484 - val_loss: 0.4522 - val_accuracy: 0.8235\n",
      "Epoch 232/1000\n",
      "Epoch 232: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3254 - accuracy: 0.8484 - val_loss: 0.4521 - val_accuracy: 0.8235\n",
      "Epoch 233/1000\n",
      "Epoch 233: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3142 - accuracy: 0.8588 - val_loss: 0.4521 - val_accuracy: 0.8235\n",
      "Epoch 234/1000\n",
      "Epoch 234: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3253 - accuracy: 0.8484 - val_loss: 0.4522 - val_accuracy: 0.8235\n",
      "Epoch 235/1000\n",
      "Epoch 235: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3232 - accuracy: 0.8484 - val_loss: 0.4521 - val_accuracy: 0.8235\n",
      "Epoch 236/1000\n",
      "Epoch 236: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3186 - accuracy: 0.8536 - val_loss: 0.4521 - val_accuracy: 0.8235\n",
      "Epoch 237/1000\n",
      "Epoch 237: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3231 - accuracy: 0.8484 - val_loss: 0.4520 - val_accuracy: 0.8235\n",
      "Epoch 238/1000\n",
      "Epoch 238: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3165 - accuracy: 0.8536 - val_loss: 0.4519 - val_accuracy: 0.8235\n",
      "Epoch 239/1000\n",
      "Epoch 239: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3185 - accuracy: 0.8536 - val_loss: 0.4518 - val_accuracy: 0.8235\n",
      "Epoch 240/1000\n",
      "Epoch 240: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3230 - accuracy: 0.8484 - val_loss: 0.4517 - val_accuracy: 0.8235\n",
      "Epoch 241/1000\n",
      "Epoch 241: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3229 - accuracy: 0.8484 - val_loss: 0.4516 - val_accuracy: 0.8235\n",
      "Epoch 242/1000\n",
      "Epoch 242: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3204 - accuracy: 0.8536 - val_loss: 0.4514 - val_accuracy: 0.8235\n",
      "Epoch 243/1000\n",
      "Epoch 243: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3117 - accuracy: 0.8588 - val_loss: 0.4514 - val_accuracy: 0.8235\n",
      "Epoch 244/1000\n",
      "Epoch 244: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3228 - accuracy: 0.8484 - val_loss: 0.4514 - val_accuracy: 0.8235\n",
      "Epoch 245/1000\n",
      "Epoch 245: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3228 - accuracy: 0.8484 - val_loss: 0.4514 - val_accuracy: 0.8235\n",
      "Epoch 246/1000\n",
      "Epoch 246: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3247 - accuracy: 0.8484 - val_loss: 0.4512 - val_accuracy: 0.8235\n",
      "Epoch 247/1000\n",
      "Epoch 247: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3247 - accuracy: 0.8484 - val_loss: 0.4510 - val_accuracy: 0.8235\n",
      "Epoch 248/1000\n",
      "Epoch 248: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3267 - accuracy: 0.8484 - val_loss: 0.4509 - val_accuracy: 0.8235\n",
      "Epoch 249/1000\n",
      "Epoch 249: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3226 - accuracy: 0.8484 - val_loss: 0.4507 - val_accuracy: 0.8235\n",
      "Epoch 250/1000\n",
      "Epoch 250: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3200 - accuracy: 0.8536 - val_loss: 0.4506 - val_accuracy: 0.8235\n",
      "Epoch 251/1000\n",
      "Epoch 251: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3225 - accuracy: 0.8484 - val_loss: 0.4504 - val_accuracy: 0.8235\n",
      "Epoch 252/1000\n",
      "Epoch 252: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3245 - accuracy: 0.8484 - val_loss: 0.4503 - val_accuracy: 0.8235\n",
      "Epoch 253/1000\n",
      "Epoch 253: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3113 - accuracy: 0.8588 - val_loss: 0.4502 - val_accuracy: 0.8235\n",
      "Epoch 254/1000\n",
      "Epoch 254: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3178 - accuracy: 0.8536 - val_loss: 0.4502 - val_accuracy: 0.8235\n",
      "Epoch 255/1000\n",
      "Epoch 255: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3178 - accuracy: 0.8536 - val_loss: 0.4502 - val_accuracy: 0.8235\n",
      "Epoch 256/1000\n",
      "Epoch 256: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3223 - accuracy: 0.8484 - val_loss: 0.4502 - val_accuracy: 0.8235\n",
      "Epoch 257/1000\n",
      "Epoch 257: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3203 - accuracy: 0.8484 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "Epoch 258/1000\n",
      "Epoch 258: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3242 - accuracy: 0.8484 - val_loss: 0.4499 - val_accuracy: 0.8235\n",
      "Epoch 259/1000\n",
      "Epoch 259: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3262 - accuracy: 0.8484 - val_loss: 0.4497 - val_accuracy: 0.8235\n",
      "Epoch 260/1000\n",
      "Epoch 260: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3242 - accuracy: 0.8484 - val_loss: 0.4496 - val_accuracy: 0.8235\n",
      "Epoch 261/1000\n",
      "Epoch 261: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3221 - accuracy: 0.8484 - val_loss: 0.4494 - val_accuracy: 0.8235\n",
      "Epoch 262/1000\n",
      "Epoch 262: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3201 - accuracy: 0.8484 - val_loss: 0.4491 - val_accuracy: 0.8235\n",
      "Epoch 263/1000\n",
      "Epoch 263: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3241 - accuracy: 0.8484 - val_loss: 0.4488 - val_accuracy: 0.8235\n",
      "Epoch 264/1000\n",
      "Epoch 264: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3240 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 265/1000\n",
      "Epoch 265: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3240 - accuracy: 0.8484 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "Epoch 266/1000\n",
      "Epoch 266: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3240 - accuracy: 0.8484 - val_loss: 0.4480 - val_accuracy: 0.8235\n",
      "Epoch 267/1000\n",
      "Epoch 267: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3193 - accuracy: 0.8536 - val_loss: 0.4478 - val_accuracy: 0.8235\n",
      "Epoch 268/1000\n",
      "Epoch 268: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3259 - accuracy: 0.8484 - val_loss: 0.4476 - val_accuracy: 0.8235\n",
      "Epoch 269/1000\n",
      "Epoch 269: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3125 - accuracy: 0.8588 - val_loss: 0.4476 - val_accuracy: 0.8235\n",
      "Epoch 270/1000\n",
      "Epoch 270: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3239 - accuracy: 0.8484 - val_loss: 0.4477 - val_accuracy: 0.8235\n",
      "Epoch 271/1000\n",
      "Epoch 271: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3191 - accuracy: 0.8536 - val_loss: 0.4477 - val_accuracy: 0.8235\n",
      "Epoch 272/1000\n",
      "Epoch 272: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3191 - accuracy: 0.8536 - val_loss: 0.4478 - val_accuracy: 0.8235\n",
      "Epoch 273/1000\n",
      "Epoch 273: Learning rate is 0.002824295312166214.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3238 - accuracy: 0.8484 - val_loss: 0.4479 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 0.0025418657809495927.\n",
      "Epoch 274/1000\n",
      "Epoch 274: Learning rate is 0.002541865687817335.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3237 - accuracy: 0.8484 - val_loss: 0.4480 - val_accuracy: 0.8235\n",
      "Epoch 275/1000\n",
      "Epoch 275: Learning rate is 0.002541865687817335.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3197 - accuracy: 0.8484 - val_loss: 0.4479 - val_accuracy: 0.8235\n",
      "Epoch 276/1000\n",
      "Epoch 276: Learning rate is 0.002541865687817335.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3170 - accuracy: 0.8536 - val_loss: 0.4479 - val_accuracy: 0.8235\n",
      "Epoch 277/1000\n",
      "Epoch 277: Learning rate is 0.002541865687817335.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3122 - accuracy: 0.8588 - val_loss: 0.4479 - val_accuracy: 0.8235\n",
      "Epoch 278/1000\n",
      "Epoch 278: Learning rate is 0.002541865687817335.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3236 - accuracy: 0.8484 - val_loss: 0.4480 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 0.0022876791190356016.\n",
      "Epoch 279/1000\n",
      "Epoch 279: Learning rate is 0.0022876791190356016.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3169 - accuracy: 0.8536 - val_loss: 0.4480 - val_accuracy: 0.8235\n",
      "Epoch 280/1000\n",
      "Epoch 280: Learning rate is 0.0022876791190356016.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3168 - accuracy: 0.8536 - val_loss: 0.4481 - val_accuracy: 0.8235\n",
      "Epoch 281/1000\n",
      "Epoch 281: Learning rate is 0.0022876791190356016.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3121 - accuracy: 0.8588 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "Epoch 282/1000\n",
      "Epoch 282: Learning rate is 0.0022876791190356016.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3167 - accuracy: 0.8536 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 283/1000\n",
      "Epoch 283: Learning rate is 0.0022876791190356016.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3167 - accuracy: 0.8536 - val_loss: 0.4487 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 0.0020589112071320416.\n",
      "Epoch 284/1000\n",
      "Epoch 284: Learning rate is 0.0020589111372828484.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3233 - accuracy: 0.8484 - val_loss: 0.4488 - val_accuracy: 0.8235\n",
      "Epoch 285/1000\n",
      "Epoch 285: Learning rate is 0.0020589111372828484.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3187 - accuracy: 0.8536 - val_loss: 0.4489 - val_accuracy: 0.8235\n",
      "Epoch 286/1000\n",
      "Epoch 286: Learning rate is 0.0020589111372828484.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3101 - accuracy: 0.8588 - val_loss: 0.4491 - val_accuracy: 0.8235\n",
      "Epoch 287/1000\n",
      "Epoch 287: Learning rate is 0.0020589111372828484.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3101 - accuracy: 0.8588 - val_loss: 0.4494 - val_accuracy: 0.8235\n",
      "Epoch 288/1000\n",
      "Epoch 288: Learning rate is 0.0020589111372828484.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3211 - accuracy: 0.8484 - val_loss: 0.4496 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00288: ReduceLROnPlateau reducing learning rate to 0.0018530200235545636.\n",
      "Epoch 289/1000\n",
      "Epoch 289: Learning rate is 0.0018530200468376279.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3166 - accuracy: 0.8536 - val_loss: 0.4497 - val_accuracy: 0.8235\n",
      "Epoch 290/1000\n",
      "Epoch 290: Learning rate is 0.0018530200468376279.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3230 - accuracy: 0.8484 - val_loss: 0.4498 - val_accuracy: 0.8235\n",
      "Epoch 291/1000\n",
      "Epoch 291: Learning rate is 0.0018530200468376279.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3165 - accuracy: 0.8536 - val_loss: 0.4499 - val_accuracy: 0.8235\n",
      "Epoch 292/1000\n",
      "Epoch 292: Learning rate is 0.0018530200468376279.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3165 - accuracy: 0.8536 - val_loss: 0.4500 - val_accuracy: 0.8235\n",
      "Epoch 293/1000\n",
      "Epoch 293: Learning rate is 0.0018530200468376279.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3185 - accuracy: 0.8536 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 0.0016677180421538651.\n",
      "Epoch 294/1000\n",
      "Epoch 294: Learning rate is 0.0016677180537953973.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3250 - accuracy: 0.8484 - val_loss: 0.4503 - val_accuracy: 0.8235\n",
      "Epoch 295/1000\n",
      "Epoch 295: Learning rate is 0.0016677180537953973.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3185 - accuracy: 0.8536 - val_loss: 0.4504 - val_accuracy: 0.8235\n",
      "Epoch 296/1000\n",
      "Epoch 296: Learning rate is 0.0016677180537953973.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3229 - accuracy: 0.8484 - val_loss: 0.4505 - val_accuracy: 0.8235\n",
      "Epoch 297/1000\n",
      "Epoch 297: Learning rate is 0.0016677180537953973.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3229 - accuracy: 0.8484 - val_loss: 0.4505 - val_accuracy: 0.8235\n",
      "Epoch 298/1000\n",
      "Epoch 298: Learning rate is 0.0016677180537953973.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3228 - accuracy: 0.8484 - val_loss: 0.4505 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00298: ReduceLROnPlateau reducing learning rate to 0.0015009462484158575.\n",
      "Epoch 299/1000\n",
      "Epoch 299: Learning rate is 0.0015009462367743254.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3228 - accuracy: 0.8484 - val_loss: 0.4505 - val_accuracy: 0.8235\n",
      "Epoch 300/1000\n",
      "Epoch 300: Learning rate is 0.0015009462367743254.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3207 - accuracy: 0.8484 - val_loss: 0.4505 - val_accuracy: 0.8235\n",
      "Epoch 301/1000\n",
      "Epoch 301: Learning rate is 0.0015009462367743254.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3249 - accuracy: 0.8484 - val_loss: 0.4504 - val_accuracy: 0.8235\n",
      "Epoch 302/1000\n",
      "Epoch 302: Learning rate is 0.0015009462367743254.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3248 - accuracy: 0.8484 - val_loss: 0.4503 - val_accuracy: 0.8235\n",
      "Epoch 303/1000\n",
      "Epoch 303: Learning rate is 0.0015009462367743254.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3227 - accuracy: 0.8484 - val_loss: 0.4502 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 0.0013508516130968928.\n",
      "Epoch 304/1000\n",
      "Epoch 304: Learning rate is 0.0013508516130968928.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3248 - accuracy: 0.8484 - val_loss: 0.4502 - val_accuracy: 0.8235\n",
      "Epoch 305/1000\n",
      "Epoch 305: Learning rate is 0.0013508516130968928.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3163 - accuracy: 0.8536 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "Epoch 306/1000\n",
      "Epoch 306: Learning rate is 0.0013508516130968928.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3248 - accuracy: 0.8484 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "Epoch 307/1000\n",
      "Epoch 307: Learning rate is 0.0013508516130968928.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3227 - accuracy: 0.8484 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "Epoch 308/1000\n",
      "Epoch 308: Learning rate is 0.0013508516130968928.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3098 - accuracy: 0.8588 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00308: ReduceLROnPlateau reducing learning rate to 0.0012157664517872036.\n",
      "Epoch 309/1000\n",
      "Epoch 309: Learning rate is 0.001215766416862607.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3142 - accuracy: 0.8536 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "Epoch 310/1000\n",
      "Epoch 310: Learning rate is 0.001215766416862607.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3206 - accuracy: 0.8484 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
      "Epoch 311/1000\n",
      "Epoch 311: Learning rate is 0.001215766416862607.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3141 - accuracy: 0.8536 - val_loss: 0.4500 - val_accuracy: 0.8235\n",
      "Epoch 312/1000\n",
      "Epoch 312: Learning rate is 0.001215766416862607.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3206 - accuracy: 0.8484 - val_loss: 0.4500 - val_accuracy: 0.8235\n",
      "Epoch 313/1000\n",
      "Epoch 313: Learning rate is 0.001215766416862607.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3226 - accuracy: 0.8484 - val_loss: 0.4499 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 0.0010941897751763463.\n",
      "Epoch 314/1000\n",
      "Epoch 314: Learning rate is 0.001094189821742475.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3226 - accuracy: 0.8484 - val_loss: 0.4498 - val_accuracy: 0.8235\n",
      "Epoch 315/1000\n",
      "Epoch 315: Learning rate is 0.001094189821742475.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3205 - accuracy: 0.8484 - val_loss: 0.4497 - val_accuracy: 0.8235\n",
      "Epoch 316/1000\n",
      "Epoch 316: Learning rate is 0.001094189821742475.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3225 - accuracy: 0.8484 - val_loss: 0.4496 - val_accuracy: 0.8235\n",
      "Epoch 317/1000\n",
      "Epoch 317: Learning rate is 0.001094189821742475.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3140 - accuracy: 0.8536 - val_loss: 0.4495 - val_accuracy: 0.8235\n",
      "Epoch 318/1000\n",
      "Epoch 318: Learning rate is 0.001094189821742475.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3161 - accuracy: 0.8536 - val_loss: 0.4495 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00318: ReduceLROnPlateau reducing learning rate to 0.0009847708395682275.\n",
      "Epoch 319/1000\n",
      "Epoch 319: Learning rate is 0.0009847708279266953.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3204 - accuracy: 0.8484 - val_loss: 0.4494 - val_accuracy: 0.8235\n",
      "Epoch 320/1000\n",
      "Epoch 320: Learning rate is 0.0009847708279266953.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3204 - accuracy: 0.8484 - val_loss: 0.4493 - val_accuracy: 0.8235\n",
      "Epoch 321/1000\n",
      "Epoch 321: Learning rate is 0.0009847708279266953.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3245 - accuracy: 0.8484 - val_loss: 0.4492 - val_accuracy: 0.8235\n",
      "Epoch 322/1000\n",
      "Epoch 322: Learning rate is 0.0009847708279266953.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3139 - accuracy: 0.8536 - val_loss: 0.4492 - val_accuracy: 0.8235\n",
      "Epoch 323/1000\n",
      "Epoch 323: Learning rate is 0.0009847708279266953.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3224 - accuracy: 0.8484 - val_loss: 0.4491 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 0.0008862937451340258.\n",
      "Epoch 324/1000\n",
      "Epoch 324: Learning rate is 0.0008862937684170902.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3224 - accuracy: 0.8484 - val_loss: 0.4490 - val_accuracy: 0.8235\n",
      "Epoch 325/1000\n",
      "Epoch 325: Learning rate is 0.0008862937684170902.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3204 - accuracy: 0.8484 - val_loss: 0.4489 - val_accuracy: 0.8235\n",
      "Epoch 326/1000\n",
      "Epoch 326: Learning rate is 0.0008862937684170902.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3224 - accuracy: 0.8484 - val_loss: 0.4488 - val_accuracy: 0.8235\n",
      "Epoch 327/1000\n",
      "Epoch 327: Learning rate is 0.0008862937684170902.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3224 - accuracy: 0.8484 - val_loss: 0.4488 - val_accuracy: 0.8235\n",
      "Epoch 328/1000\n",
      "Epoch 328: Learning rate is 0.0008862937684170902.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3203 - accuracy: 0.8484 - val_loss: 0.4487 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00328: ReduceLROnPlateau reducing learning rate to 0.0007976643915753812.\n",
      "Epoch 329/1000\n",
      "Epoch 329: Learning rate is 0.0007976643973961473.\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3094 - accuracy: 0.8588 - val_loss: 0.4486 - val_accuracy: 0.8235\n",
      "Epoch 330/1000\n",
      "Epoch 330: Learning rate is 0.0007976643973961473.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3138 - accuracy: 0.8536 - val_loss: 0.4486 - val_accuracy: 0.8235\n",
      "Epoch 331/1000\n",
      "Epoch 331: Learning rate is 0.0007976643973961473.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3244 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 332/1000\n",
      "Epoch 332: Learning rate is 0.0007976643973961473.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3203 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 333/1000\n",
      "Epoch 333: Learning rate is 0.0007976643973961473.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3243 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00333: ReduceLROnPlateau reducing learning rate to 0.0007178979576565325.\n",
      "Epoch 334/1000\n",
      "Epoch 334: Learning rate is 0.0007178979576565325.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3243 - accuracy: 0.8484 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Epoch 335/1000\n",
      "Epoch 335: Learning rate is 0.0007178979576565325.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3113 - accuracy: 0.8588 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Epoch 336/1000\n",
      "Epoch 336: Learning rate is 0.0007178979576565325.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3223 - accuracy: 0.8484 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Epoch 337/1000\n",
      "Epoch 337: Learning rate is 0.0007178979576565325.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3223 - accuracy: 0.8484 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Epoch 338/1000\n",
      "Epoch 338: Learning rate is 0.0007178979576565325.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3223 - accuracy: 0.8484 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00338: ReduceLROnPlateau reducing learning rate to 0.0006461081618908793.\n",
      "Epoch 339/1000\n",
      "Epoch 339: Learning rate is 0.0006461081793531775.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3202 - accuracy: 0.8484 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Epoch 340/1000\n",
      "Epoch 340: Learning rate is 0.0006461081793531775.\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3202 - accuracy: 0.8484 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "Epoch 341/1000\n",
      "Epoch 341: Learning rate is 0.0006461081793531775.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3222 - accuracy: 0.8484 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "Epoch 342/1000\n",
      "Epoch 342: Learning rate is 0.0006461081793531775.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3157 - accuracy: 0.8536 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "Epoch 343/1000\n",
      "Epoch 343: Learning rate is 0.0006461081793531775.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3092 - accuracy: 0.8588 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 0.0005814973614178598.\n",
      "Epoch 344/1000\n",
      "Epoch 344: Learning rate is 0.0005814973847009242.\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3182 - accuracy: 0.8484 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "Epoch 345/1000\n",
      "Epoch 345: Learning rate is 0.0005814973847009242.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3177 - accuracy: 0.8536 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "Epoch 346/1000\n",
      "Epoch 346: Learning rate is 0.0005814973847009242.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3112 - accuracy: 0.8588 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "Epoch 347/1000\n",
      "Epoch 347: Learning rate is 0.0005814973847009242.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3177 - accuracy: 0.8536 - val_loss: 0.4482 - val_accuracy: 0.8235\n",
      "Epoch 348/1000\n",
      "Epoch 348: Learning rate is 0.0005814973847009242.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3202 - accuracy: 0.8484 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00348: ReduceLROnPlateau reducing learning rate to 0.0005233476462308317.\n",
      "Epoch 349/1000\n",
      "Epoch 349: Learning rate is 0.0005233476404100657.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3222 - accuracy: 0.8484 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "Epoch 350/1000\n",
      "Epoch 350: Learning rate is 0.0005233476404100657.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3157 - accuracy: 0.8536 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "Epoch 351/1000\n",
      "Epoch 351: Learning rate is 0.0005233476404100657.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3112 - accuracy: 0.8588 - val_loss: 0.4483 - val_accuracy: 0.8235\n",
      "Epoch 352/1000\n",
      "Epoch 352: Learning rate is 0.0005233476404100657.\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3177 - accuracy: 0.8536 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Epoch 353/1000\n",
      "Epoch 353: Learning rate is 0.0005233476404100657.\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3177 - accuracy: 0.8536 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00353: ReduceLROnPlateau reducing learning rate to 0.0004710128763690591.\n",
      "Epoch 354/1000\n",
      "Epoch 354: Learning rate is 0.000471012870548293.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3177 - accuracy: 0.8536 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 355/1000\n",
      "Epoch 355: Learning rate is 0.000471012870548293.\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3201 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 356/1000\n",
      "Epoch 356: Learning rate is 0.000471012870548293.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3177 - accuracy: 0.8536 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 357/1000\n",
      "Epoch 357: Learning rate is 0.000471012870548293.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3201 - accuracy: 0.8484 - val_loss: 0.4486 - val_accuracy: 0.8235\n",
      "Epoch 358/1000\n",
      "Epoch 358: Learning rate is 0.000471012870548293.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3241 - accuracy: 0.8484 - val_loss: 0.4486 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00358: ReduceLROnPlateau reducing learning rate to 0.0004239115834934637.\n",
      "Epoch 359/1000\n",
      "Epoch 359: Learning rate is 0.00042391158058308065.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3201 - accuracy: 0.8484 - val_loss: 0.4486 - val_accuracy: 0.8235\n",
      "Epoch 360/1000\n",
      "Epoch 360: Learning rate is 0.00042391158058308065.\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3221 - accuracy: 0.8484 - val_loss: 0.4486 - val_accuracy: 0.8235\n",
      "Epoch 361/1000\n",
      "Epoch 361: Learning rate is 0.00042391158058308065.\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3136 - accuracy: 0.8536 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 362/1000\n",
      "Epoch 362: Learning rate is 0.00042391158058308065.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3241 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 363/1000\n",
      "Epoch 363: Learning rate is 0.00042391158058308065.\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3180 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 00363: ReduceLROnPlateau reducing learning rate to 0.0003815204225247726.\n",
      "Epoch 364/1000\n",
      "Epoch 364: Learning rate is 0.00038152042543515563.\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3200 - accuracy: 0.8484 - val_loss: 0.4485 - val_accuracy: 0.8235\n",
      "Epoch 365/1000\n",
      "Epoch 365: Learning rate is 0.00038152042543515563.\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3221 - accuracy: 0.8484 - val_loss: 0.4484 - val_accuracy: 0.8235\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00365: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Pass the callback to the fit method\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks  # include it here\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:42.710924733Z",
     "start_time": "2024-02-16T11:44:29.002456713Z"
    }
   },
   "id": "bd3f41f1f688e43"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: 1.9466 - accuracy: 0.8095\n",
      "Test accuracy: 0.8095238208770752\n",
      "Test loss: 1.9465711116790771\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:45.010470270Z",
     "start_time": "2024-02-16T11:44:44.976890579Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 7,  0,  0],\n       [ 1,  0,  3],\n       [ 0,  0, 10]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "confusion_matrix(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:44:51.893225504Z",
     "start_time": "2024-02-16T11:44:51.832366175Z"
    }
   },
   "id": "1352cb1eded44f03",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plotting\n",
    "# plt.figure(figsize=(8, 8))\n",
    "plt.plot(lr_tracker.learning_rates)\n",
    "plt.title(\"Learning Rate over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.grid(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:45:18.325493651Z",
     "start_time": "2024-02-16T11:45:16.907266268Z"
    }
   },
   "id": "1d82547b5da98e0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:47:23.817471: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/adam/FurtherResearch/Model/Exoskeleton/Specific/exo_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "tf.keras.models.save_model(model, '/home/adam/FurtherResearch/Model/Exoskeleton/Specific/exo_model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:47:25.058685626Z",
     "start_time": "2024-02-16T11:47:23.707556623Z"
    }
   },
   "id": "971afc8f1ecd8193"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_0_input (InputLayer)   [(None, 63)]              0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 128)               8192      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 16,643\n",
      "Trainable params: 16,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# model.summary()\n",
    "# Assume 'original_model' is your pre-trained model\n",
    "model_without_softmax = Model(inputs=model.input, \n",
    "                              outputs=model.get_layer(\"dense_2\").output)\n",
    "model_without_softmax.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:47:33.070412965Z",
     "start_time": "2024-02-16T11:47:33.026840309Z"
    }
   },
   "id": "9445a32c75176e7b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/adam/FurtherResearch/Model/Exoskeleton/Specific/exo_model_without_softmax/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "tf.keras.models.save_model(model_without_softmax, '/home/adam/FurtherResearch/Model/Exoskeleton/Specific/exo_model_without_softmax')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T11:47:42.775677573Z",
     "start_time": "2024-02-16T11:47:42.556909300Z"
    }
   },
   "id": "61da15b78744777b"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[7.91898880e+01, 5.57430792e+00, 6.68306274e+01],\n       [0.00000000e+00, 3.90658832e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 9.56836033e+00, 0.00000000e+00],\n       [0.00000000e+00, 9.63489592e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.05130844e+01, 2.18199577e+01, 6.36735821e+00],\n       [0.00000000e+00, 2.25438766e+01, 0.00000000e+00],\n       [6.59429092e+01, 1.70275002e+01, 5.27950058e+01],\n       [6.52022781e+01, 1.99708233e+01, 5.08618851e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [5.90349998e+01, 1.78758469e+01, 4.42702179e+01],\n       [0.00000000e+00, 1.00491638e+01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.45404434e+01, 2.00261917e+01, 1.99619961e+01],\n       [0.00000000e+00, 2.16502213e+00, 0.00000000e+00],\n       [1.49675888e+02, 3.46249886e+01, 1.25935844e+02],\n       [0.00000000e+00, 1.08430662e+01, 0.00000000e+00],\n       [8.10442200e+01, 2.38256493e+01, 6.35654678e+01],\n       [5.99550247e+01, 1.94702454e+01, 4.70332260e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.13088453e+00, 0.00000000e+00],\n       [4.14269104e+01, 1.21164799e+01, 3.25735779e+01],\n       [0.00000000e+00, 6.08060312e+00, 0.00000000e+00],\n       [0.00000000e+00, 5.10136414e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [9.67972565e+01, 3.05324192e+01, 8.06252289e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [6.63790817e+01, 1.90754166e+01, 5.11629028e+01],\n       [0.00000000e+00, 3.17098069e+00, 0.00000000e+00],\n       [3.38438072e+01, 1.42922382e+01, 2.48223038e+01],\n       [0.00000000e+00, 4.62081194e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 5.81919193e+00, 0.00000000e+00],\n       [9.12653427e+01, 0.00000000e+00, 7.89331207e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 2.13683224e+00, 0.00000000e+00],\n       [7.52585373e+01, 2.10117760e+01, 6.43474503e+01],\n       [0.00000000e+00, 7.71293592e+00, 0.00000000e+00],\n       [0.00000000e+00, 6.32173395e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.17875376e+01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 4.68138266e+00, 0.00000000e+00],\n       [5.09792633e+01, 1.84472122e+01, 4.00433846e+01],\n       [0.00000000e+00, 1.16123753e+01, 0.00000000e+00],\n       [5.52049637e+01, 1.61420593e+01, 4.46497765e+01],\n       [4.09868774e+01, 1.21371727e+01, 3.24046097e+01],\n       [0.00000000e+00, 3.63345194e+00, 0.00000000e+00],\n       [7.50502634e+00, 3.10184517e+01, 4.10724974e+00],\n       [7.27775497e+01, 2.58337898e+01, 5.58700409e+01],\n       [3.58186493e+01, 1.39175682e+01, 2.54058094e+01],\n       [2.32790489e+01, 2.59444809e+00, 1.74767208e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.17343620e+02, 2.55190830e+01, 1.03485344e+02],\n       [0.00000000e+00, 9.24306512e-01, 0.00000000e+00],\n       [1.40316605e+02, 1.63054047e+01, 1.25644218e+02],\n       [1.45959549e+02, 2.94769821e+01, 1.25607208e+02],\n       [0.00000000e+00, 4.44881248e+00, 0.00000000e+00],\n       [4.33991241e+01, 9.33581543e+00, 3.47576523e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [4.22476845e+01, 1.69905701e+01, 3.22868118e+01],\n       [0.00000000e+00, 1.22669029e+01, 0.00000000e+00],\n       [0.00000000e+00, 6.94787502e+00, 0.00000000e+00],\n       [1.53056351e+02, 2.10454025e+01, 1.35984604e+02],\n       [1.27088913e+02, 3.05119133e+01, 1.10100502e+02],\n       [0.00000000e+00, 6.31912804e+00, 0.00000000e+00],\n       [0.00000000e+00, 7.67586184e+00, 0.00000000e+00],\n       [3.47888718e+01, 4.18330050e+00, 2.68960629e+01],\n       [0.00000000e+00, 4.38828975e-01, 0.00000000e+00],\n       [9.75256271e+01, 5.96733379e+00, 8.29090424e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 4.44261885e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [4.30506401e+01, 1.90791283e+01, 3.24760017e+01],\n       [0.00000000e+00, 5.01938009e+00, 0.00000000e+00],\n       [0.00000000e+00, 2.56044292e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.24470528e+02, 2.80709801e+01, 1.09591919e+02],\n       [4.77738266e+01, 0.00000000e+00, 3.90330429e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 5.97510958e+00, 0.00000000e+00],\n       [6.08356361e+01, 1.90026283e+01, 5.03472137e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [8.90541382e+01, 4.24789696e+01, 5.99780502e+01],\n       [0.00000000e+00, 3.63595295e+00, 0.00000000e+00],\n       [2.20917999e+02, 4.23916435e+01, 1.89910431e+02],\n       [0.00000000e+00, 7.43080282e+00, 0.00000000e+00],\n       [0.00000000e+00, 3.35619259e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.49282014e+00, 0.00000000e+00],\n       [3.47651749e+01, 1.79060421e+01, 2.30381145e+01],\n       [9.54885178e+01, 0.00000000e+00, 7.69791183e+01],\n       [2.63136921e+01, 9.90148067e+00, 1.77368603e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.31954071e+02, 1.66453247e+01, 1.18146088e+02],\n       [0.00000000e+00, 3.13031912e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.19982246e+02, 3.19591255e+01, 9.65410156e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [9.27786331e+01, 2.43698597e+01, 7.95628967e+01],\n       [4.81282196e+01, 2.35956879e+01, 3.57844467e+01],\n       [7.27855911e+01, 2.87626953e+01, 5.95355530e+01],\n       [5.31254044e+01, 1.03937035e+01, 4.18373947e+01],\n       [2.64360905e+01, 1.13084545e+01, 1.88435955e+01],\n       [4.36284637e+01, 1.57054892e+01, 3.25612259e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 2.09191155e+00, 0.00000000e+00],\n       [0.00000000e+00, 4.84042215e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.12126637e+00, 1.96055412e+01, 0.00000000e+00],\n       [3.46745300e+01, 7.36459351e+00, 2.84451180e+01],\n       [0.00000000e+00, 6.70250082e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [5.96764145e+01, 2.25087357e+01, 4.54719849e+01],\n       [4.56122360e+01, 1.64567642e+01, 3.48950119e+01],\n       [8.00139160e+01, 2.64989662e+01, 6.01479645e+01],\n       [1.32840210e+02, 3.53330193e+01, 1.11737946e+02],\n       [1.43377548e+02, 2.08186302e+01, 1.26568024e+02],\n       [4.11074982e+01, 2.80552387e+00, 3.28321877e+01],\n       [0.00000000e+00, 2.23065996e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [9.58649521e+01, 2.35653496e+01, 7.80518188e+01],\n       [0.00000000e+00, 2.22625470e+00, 0.00000000e+00],\n       [9.34331436e+01, 5.29867554e+00, 8.01641235e+01],\n       [0.00000000e+00, 8.35549545e+00, 0.00000000e+00],\n       [8.39136658e+01, 2.39025726e+01, 6.69991150e+01],\n       [6.27387810e+01, 2.24502659e+00, 5.59911499e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 6.25999165e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 7.01340437e+00, 0.00000000e+00],\n       [0.00000000e+00, 3.07890725e+00, 0.00000000e+00],\n       [9.47310410e+01, 1.82698879e+01, 8.19715881e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [4.02712402e+01, 1.14962463e+01, 3.19702015e+01],\n       [4.26644287e+01, 1.66430054e+01, 3.11320629e+01],\n       [4.80641508e+00, 2.44732265e+01, 2.33881164e+00],\n       [4.97464371e+01, 3.53606224e+00, 4.47060776e+01],\n       [3.14637985e+01, 1.31659660e+01, 2.37390156e+01],\n       [0.00000000e+00, 9.29631805e+00, 0.00000000e+00],\n       [7.66834946e+01, 1.99726963e+01, 5.90892296e+01],\n       [2.57349854e+01, 0.00000000e+00, 2.15979252e+01],\n       [7.54132233e+01, 8.86683941e+00, 6.30738068e+01],\n       [0.00000000e+00, 3.31858850e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.62787819e+01, 1.70594120e+01, 1.64683971e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.99962158e+01, 1.61330795e+01, 2.05886898e+01],\n       [6.21927032e+01, 3.86622262e+00, 5.45226059e+01],\n       [1.01568169e+02, 2.91566229e+00, 8.53578720e+01],\n       [7.32183304e+01, 2.00263176e+01, 5.72897263e+01],\n       [0.00000000e+00, 5.96251154e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.49046326e+02, 3.92215424e+01, 1.27941986e+02],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 5.24081326e+00, 0.00000000e+00],\n       [1.36444412e+02, 3.39976768e+01, 1.15721100e+02],\n       [3.74047928e+01, 1.91616917e+01, 2.56294479e+01],\n       [2.44863739e+01, 1.28985138e+01, 1.72310429e+01],\n       [1.02996483e+02, 2.57827187e+01, 9.03926849e+01],\n       [9.30058060e+01, 2.24670811e+01, 7.41154633e+01],\n       [1.57784500e+02, 2.18118477e+01, 1.40912262e+02],\n       [1.43186325e+02, 3.52042732e+01, 1.21823517e+02],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.17151575e+01, 6.96941948e+00, 5.31190395e+00],\n       [0.00000000e+00, 3.22134876e+00, 0.00000000e+00],\n       [4.99864731e+01, 6.62245417e+00, 4.11240044e+01],\n       [3.85775108e+01, 1.22046804e+01, 3.02527485e+01],\n       [2.17183426e+02, 2.64672089e+01, 1.86885712e+02],\n       [2.99463692e+01, 1.09633799e+01, 2.33181286e+01],\n       [2.49821815e+01, 5.60227489e+00, 1.76061554e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.59955204e+00, 4.48289537e+00, 0.00000000e+00],\n       [8.07290421e+01, 3.36475563e+01, 6.07211456e+01],\n       [0.00000000e+00, 2.76405067e+01, 0.00000000e+00],\n       [0.00000000e+00, 7.89778519e+00, 0.00000000e+00],\n       [0.00000000e+00, 9.49864006e+00, 0.00000000e+00],\n       [3.92977333e+01, 1.15418148e+01, 3.15516052e+01],\n       [0.00000000e+00, 7.27877235e+00, 0.00000000e+00],\n       [5.81858101e+01, 1.91847191e+01, 4.36680450e+01],\n       [0.00000000e+00, 7.16504991e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 7.16782856e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.73026371e+01, 1.80406303e+01, 7.97268009e+00],\n       [4.32202972e+02, 1.13745705e+02, 3.80422760e+02],\n       [0.00000000e+00, 1.55239372e+01, 0.00000000e+00],\n       [9.06872101e+01, 6.79589510e+00, 7.67478790e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 2.07448125e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.55139008e+01, 0.00000000e+00],\n       [0.00000000e+00, 2.62124926e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 2.17390633e+00, 0.00000000e+00],\n       [0.00000000e+00, 6.29328346e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 5.78934145e+00, 0.00000000e+00],\n       [0.00000000e+00, 6.05127621e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.30802333e-01, 1.92365627e+01, 0.00000000e+00],\n       [6.92252808e+01, 2.11387463e+01, 5.30371094e+01],\n       [1.11332527e+02, 1.66715736e+01, 9.56428986e+01],\n       [0.00000000e+00, 1.17336998e+01, 0.00000000e+00],\n       [6.74232483e+01, 2.07073231e+01, 5.29878159e+01],\n       [6.39651260e+01, 2.06625214e+01, 5.19123421e+01],\n       [3.73604774e+01, 1.15611992e+01, 2.96032085e+01],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.00666641e+02, 1.88267155e+01, 1.75686996e+02],\n       [4.17098160e+01, 8.59470081e+00, 3.45317497e+01],\n       [2.94849815e+01, 1.27244129e+01, 2.13794289e+01],\n       [0.00000000e+00, 1.05274849e+01, 0.00000000e+00],\n       [0.00000000e+00, 5.20507288e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [5.69182730e+00, 1.80576336e+00, 6.58461988e-01],\n       [7.03150864e+01, 2.12193050e+01, 5.61152496e+01],\n       [6.96264038e+01, 2.63447781e+01, 5.72682877e+01],\n       [0.00000000e+00, 1.65734329e+01, 0.00000000e+00],\n       [0.00000000e+00, 1.05145302e+01, 0.00000000e+00],\n       [0.00000000e+00, 1.93371344e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.11417758e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.23747044e+01, 0.00000000e+00],\n       [9.21455917e+01, 3.07543468e+01, 7.36630249e+01],\n       [5.97720985e+01, 1.90387421e+01, 4.62519875e+01],\n       [0.00000000e+00, 2.99120235e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 6.67055750e+00, 0.00000000e+00],\n       [1.48485321e+02, 1.86015682e+01, 1.31223846e+02],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 2.99102669e+01, 0.00000000e+00],\n       [0.00000000e+00, 4.36551905e+00, 0.00000000e+00],\n       [4.26665115e+00, 0.00000000e+00, 8.82118344e-02]], dtype=float32)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "loaded_model = tf.keras.models.load_model('exo_model_without_softmax')\n",
    "predictions = loaded_model.predict(X_test)\n",
    "predictions\n",
    "# predicted_classes = np.argmax(predictions, axis=1)\n",
    "# print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:48:21.463875700Z",
     "start_time": "2023-11-30T20:48:21.062392200Z"
    }
   },
   "id": "7240933c75a66b7"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]], dtype=float32)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T20:47:32.983155600Z",
     "start_time": "2023-11-30T20:47:32.926021200Z"
    }
   },
   "id": "cd3fe90cc0416d13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7e106bcce6546135"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
